import os
import sys
import json
import re
import base64
import mimetypes
import pandas as pd
from typing import List, Dict, Any, Optional, Union, Tuple
from dataclasses import asdict

# å‡è®¾æ–‡ä»¶ç»“æ„ï¼Œç¡®ä¿å¯ä»¥å¯¼å…¥ src ä¸‹çš„æ¨¡å—
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "../../")))

from openai import OpenAI
from src.loaders.base_loader import BaseDataLoader, StandardSample, PageElement
from src.agents.RAGAgent import RAGAgent

# å¤ç”¨ utils ä¸­çš„æ­£åˆ™åŒ¹é…ï¼Œæˆ–è€…é‡æ–°å®šä¹‰ä»¥ç¡®ä¿å¥å£®æ€§
TOOL_CALL_RE = re.compile(r"<tool_call>\s*(\{.*?\})\s*</tool_call>", re.DOTALL)

def local_image_to_data_url(path: str) -> str:
    """
    å°†æœ¬åœ°å›¾ç‰‡è½¬æ¢ä¸º Data URL (Base64)
    """
    if not path or not os.path.exists(path):
        return ""
    
    mime, _ = mimetypes.guess_type(path)
    if mime is None:
        mime = "image/png"

    try:
        with open(path, "rb") as f:
            b64 = base64.b64encode(f.read()).decode("utf-8")
        return f"data:{mime};base64,{b64}"
    except Exception as e:
        print(f"Error encoding image {path}: {e}")
        return ""

# --- System Prompt å®šä¹‰ ---
AGENTIC_SYSTEM_PROMPT = """ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½æ–‡æ¡£é—®ç­”åŠ©æ‰‹ (Agentic RAG Agent)ã€‚ä½ çš„ç›®æ ‡æ˜¯é€šè¿‡æ£€ç´¢æ–‡æ¡£ä¸­çš„è¯æ®æ¥å‡†ç¡®å›ç­”ç”¨æˆ·çš„å¤æ‚é—®é¢˜ã€‚

### ä½ çš„èƒ½åŠ›ä¸å·¥å…·
ä½ å¯ä»¥ä½¿ç”¨å·¥å…· **`search_evidence_tool`** æ¥ä»æ–‡æ¡£åº“ä¸­æ£€ç´¢ä¿¡æ¯ã€‚

- **åŠŸèƒ½**: è¾“å…¥ä¸€ä¸ªå…·ä½“çš„æŸ¥è¯¢å­—ç¬¦ä¸²ï¼ˆSub-queryï¼‰ï¼Œç³»ç»Ÿå°†æ‰§è¡Œæ£€ç´¢ã€é‡æ’åºå’Œç»†ç²’åº¦ä¿¡æ¯æå–ï¼Œå¹¶è¿”å›ç›¸å…³çš„è¯æ®ç‰‡æ®µã€‚
- **ä½•æ—¶ä½¿ç”¨**: 
  - å½“ç”¨æˆ·çš„é—®é¢˜æ— æ³•ç›´æ¥å‡­ç©ºå›ç­”æ—¶ã€‚
  - å½“éœ€è¦æŸ¥æ‰¾å…·ä½“çš„äº‹å®ã€æ•°æ®ã€æ¡æ¬¾æˆ–å¯¹æ¯”ä¿¡æ¯æ—¶ã€‚
  - å¦‚æœç”¨æˆ·çš„é—®é¢˜åŒ…å«å¤šä¸ªæ–¹é¢ï¼ˆä¾‹å¦‚â€œæ¯”è¾ƒ A å’Œ B çš„æ”¶å…¥â€ï¼‰ï¼Œè¯·**å°†é—®é¢˜åˆ†è§£**ä¸ºç®€å•çš„å­æŸ¥è¯¢ï¼ˆä¾‹å¦‚å…ˆæŸ¥â€œA çš„æ”¶å…¥â€ï¼Œå†æŸ¥â€œB çš„æ”¶å…¥â€ï¼‰ï¼Œåˆ†æ­¥è°ƒç”¨å·¥å…·ã€‚

### å·¥å…·è°ƒç”¨æ ¼å¼
è¯·ä¸¥æ ¼ä½¿ç”¨ä»¥ä¸‹ JSON æ ¼å¼åŒ…è£¹åœ¨ XML æ ‡ç­¾ä¸­è¿›è¡Œè°ƒç”¨ï¼š

<tool_call>
{"name": "search_evidence_tool", "arguments": {"query": "<ä½ çš„æœç´¢å…³é”®è¯æˆ–å­é—®é¢˜>"}}
</tool_call>

### æ€è€ƒä¸å›å¤æµç¨‹ (ReAct èŒƒå¼)
1. **Thought**: æ€è€ƒå½“å‰éœ€è¦ä»€ä¹ˆä¿¡æ¯æ¥å›ç­”ç”¨æˆ·é—®é¢˜ï¼Œæ˜¯å¦éœ€è¦åˆ†è§£é—®é¢˜ã€‚
2. **Tool Call**: å¦‚æœéœ€è¦ä¿¡æ¯ï¼Œç”Ÿæˆå·¥å…·è°ƒç”¨ã€‚
3. **Observation**: (ç³»ç»Ÿä¼šå°†å·¥å…·è¿”å›çš„ç»“æœç”¨ `<tool_response></tool_response>` åŒ…è£¹åæ’å…¥å¯¹è¯)ã€‚
4. **Repeat**: æ ¹æ®è§‚å¯Ÿåˆ°çš„ç»“æœï¼Œå†³å®šæ˜¯å¦éœ€è¦è¿›ä¸€æ­¥æœç´¢ã€‚
5. **Final Answer**: å½“æ”¶é›†åˆ°è¶³å¤Ÿçš„ä¿¡æ¯åï¼Œæˆ–è€…ç¡®å®šæ— æ³•æ‰¾åˆ°ä¿¡æ¯æ—¶ï¼Œç›´æ¥è¾“å‡ºæœ€ç»ˆç­”æ¡ˆï¼ˆä¸è¦å†åŒ…å« tool_callï¼‰ã€‚

### è¾“å‡ºç¤ºä¾‹
User: 2023å¹´å’Œ2024å¹´çš„æ€»æ”¶å…¥åˆ†åˆ«æ˜¯å¤šå°‘ï¼Ÿ
Assistant: 
Thought: ç”¨æˆ·è¯¢é—®ä¸¤å¹´çš„æ”¶å…¥ï¼Œæˆ‘éœ€è¦åˆ†åˆ«æ£€ç´¢2023å¹´å’Œ2024å¹´çš„æ”¶å…¥æ•°æ®ã€‚é¦–å…ˆæ£€ç´¢2023å¹´çš„ã€‚
<tool_call>
{"name": "search_evidence_tool", "arguments": {"query": "2023å¹´ æ€»æ”¶å…¥"}}
</tool_call>
... (ç­‰å¾…å·¥å…·è¿”å›) ...
"""

class AgenticRAGAgent(RAGAgent):
    """
    åŸºäº ReAct èŒƒå¼çš„ Agentic RAGã€‚
    ç»§æ‰¿è‡ª RAGAgent ä»¥ä¿æŒæ¥å£å…¼å®¹ï¼Œä½†é‡å†™äº† process_sample é€»è¾‘ä»¥å¼•å…¥ LLM é©±åŠ¨çš„å¾ªç¯ã€‚
    åŒ…å«ç¼“å­˜æœºåˆ¶ä»¥æ”¯æŒæ¨ç†ä¸è¯„ä¼°åˆ†ç¦»ã€‚
    """

    def __init__(
        self, 
        loader: BaseDataLoader, 
        base_url: str, 
        api_key: str, 
        model_name: str, 
        max_rounds: int,
        cache_dir: str = "./cache_results"  # æ–°å¢ç¼“å­˜ç›®å½•å‚æ•°
    ):
        """
        :param loader: æ•°æ®é›†åŠ è½½å™¨ (FinRAGLoader, MMLongLoader ç­‰)ï¼Œç”¨äºæä¾› pipeline ä½œä¸ºæ£€ç´¢å·¥å…·ã€‚
        :param base_url: LLM API åœ°å€ã€‚
        :param api_key: LLM API Keyã€‚
        :param model_name: æ¨¡å‹åç§°ã€‚
        :param max_rounds: æœ€å¤§äº¤äº’/æ€è€ƒè½®æ•°ï¼Œé˜²æ­¢æ­»å¾ªç¯ã€‚
        :param cache_dir: ç»“æœç¼“å­˜ç›®å½•ï¼Œç”¨äºæŒä¹…åŒ–å­˜å‚¨æ¨ç†ç»“æœã€‚
        """
        super().__init__(loader)
        self.client = OpenAI(base_url=base_url, api_key=api_key)
        self.model_name = model_name
        self.max_rounds = max_rounds
        self.cache_dir = cache_dir
        
        if not os.path.exists(self.cache_dir):
            os.makedirs(self.cache_dir)

    def extract_tool_call(self, text: str) -> Optional[Dict]:
        """ä»æ–‡æœ¬ä¸­æå– JSON æ ¼å¼çš„å·¥å…·è°ƒç”¨"""
        match = TOOL_CALL_RE.search(text)
        if not match:
            return None
        try:
            return json.loads(match.group(1).strip())
        except json.JSONDecodeError:
            print(f"Error parsing tool call JSON: {match.group(1)}")
            return None

    def _execute_tool(self, tool_name: str, args: Dict, sample: StandardSample) -> Union[str, List[Dict[str, Any]]]:
        """
        æ‰§è¡Œå·¥å…·è°ƒç”¨ã€‚
        å°†æ£€ç´¢åˆ°çš„ PageElement è½¬æ¢ä¸ºå¤šæ¨¡æ€æ¶ˆæ¯å†…å®¹ (List[Dict])ã€‚
        åŒ…å«: é¡µé¢åŸå›¾(Page Image) + è¯æ®æˆªå›¾(Crop) + æ–‡æœ¬å†…å®¹(Content)ã€‚
        """
        if tool_name == "search_evidence_tool":
            query = args.get("query", "")
            if not query:
                return "Error: Query cannot be empty."
            
            print(f"  [Agent Action] Searching for: {query}")
            
            try:
                elements = self.loader.pipeline(query=query, image_paths=[sample.data_source], top_k=10)
                
                if not elements:
                    return "No relevant evidence found."
                
                # æ›´æ–° retrieved_elements
                current_elements = sample.extra_info.get('retrieved_elements', [])
                # é¿å…é‡å¤æ·»åŠ  (ç®€å•é€»è¾‘)
                sample.extra_info['retrieved_elements'] = current_elements + elements
                
                # æ„é€ å¤šæ¨¡æ€ Tool Response
                content_list = []
                content_list.append({"type": "text", "text": "<tool_response>\nFound the following evidence:\n"})
                
                for i, el in enumerate(elements):
                    content_list.append({"type": "text", "text": f"\n--- Evidence {i+1} ---\n"})
                    
                    # 1. å…ƒç´ æ‰€åœ¨é¡µé¢å›¾åƒ (Page Image)
                    if el.corpus_id:
                        img_url = local_image_to_data_url(el.corpus_id)
                        if img_url:
                             content_list.append({"type": "text", "text": f"[Page Source: {os.path.basename(el.corpus_id)}]\n"})
                             content_list.append({"type": "image_url", "image_url": {"url": img_url}})
                    
                    # 2. å…ƒç´ æˆªå›¾ (Evidence Crop)
                    # å¦‚æœ crop_path å­˜åœ¨ä¸”ä¸ corpus_id ä¸åŒï¼ˆé¿å…é‡å¤å±•ç¤ºå…¨é¡µï¼‰ï¼Œåˆ™å±•ç¤º
                    if el.crop_path and el.crop_path != el.corpus_id:
                        crop_url = local_image_to_data_url(el.crop_path)
                        if crop_url:
                            content_list.append({"type": "text", "text": "\n[Evidence Detail Crop]\n"})
                            content_list.append({"type": "image_url", "image_url": {"url": crop_url}})
                    
                    # 3. å…ƒç´ æ–‡æœ¬å†…å®¹
                    content_list.append({"type": "text", "text": f"\nContent: {el.content}\n"})
                
                content_list.append({"type": "text", "text": "\n</tool_response>"})
                return content_list
            
            except Exception as e:
                return f"Error during retrieval: {str(e)}"
        
        else:
            return f"Error: Unknown tool '{tool_name}'."

    def run_agent_loop(self, sample: StandardSample) -> Tuple[str, List[Dict]]:
        """
        æ‰§è¡Œ ReAct å¾ªç¯çš„æ ¸å¿ƒå¼‚æ­¥æ–¹æ³•ã€‚
        è¿”å›: (final_answer, full_messages_history)
        """
        messages = [
            {"role": "system", "content": AGENTIC_SYSTEM_PROMPT},
            {"role": "user", "content": sample.query}
        ]

        final_answer = ""
        
        for i in range(self.max_rounds):
            # 1. LLM ç”Ÿæˆ Thought å’Œ Potential Tool Call
            try:
                response = self.client.chat.completions.create(
                    model=self.model_name,
                    messages=messages,
                    temperature=1.0
                )
                content = response.choices[0].message.content
                messages.append({"role": "assistant", "content": content})
                print(f"Round {i+1} Assistant: {content[:100]}...") 

            except Exception as e:
                print(f"LLM API Error: {e}")
                final_answer = "Error during generation."
                break

            # 2. æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨
            tool_call_dict = self.extract_tool_call(content)
            
            if tool_call_dict:
                # 3. æ‰§è¡Œå·¥å…·
                tool_name = tool_call_dict.get("name")
                tool_args = tool_call_dict.get("arguments", {})
                
                tool_result = self._execute_tool(tool_name, tool_args, sample)
                
                # 4. æ„é€  Tool Response
                if isinstance(tool_result, list):
                    # å¦‚æœè¿”å›æ˜¯åˆ—è¡¨ï¼Œè¯´æ˜æ˜¯æ„é€ å¥½çš„å¤šæ¨¡æ€æ¶ˆæ¯
                    messages.append({"role": "user", "content": tool_result})
                else:
                    # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼ˆé€šå¸¸æ˜¯é”™è¯¯ä¿¡æ¯ï¼‰ï¼ŒåŒ…è£…ä¸ºæ–‡æœ¬æ¶ˆæ¯
                    tool_msg_content = f"<tool_response>\n{tool_result}\n</tool_response>"
                    messages.append({"role": "user", "content": tool_msg_content})
                
            else:
                # æ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œè¯´æ˜ LLM ç”Ÿæˆäº†æœ€ç»ˆå›å¤
                final_answer = content
                break
        
        if not final_answer and messages[-1]["role"] == "assistant":
            final_answer = messages[-1]["content"]

        return final_answer, messages

    def process_sample(self, sample: StandardSample) -> StandardSample:
        """
        å¤„ç†æ ·æœ¬ã€‚
        1. æ£€æŸ¥ç¼“å­˜ï¼šå¦‚æœå­˜åœ¨ç¼“å­˜ç»“æœï¼Œç›´æ¥è¯»å–å¹¶è·³è¿‡æ¨ç†ã€‚
        2. å¦‚æœæ— ç¼“å­˜ï¼šæ‰§è¡Œ ReAct Agent Loopï¼Œå¹¶å°†ç»“æœå†™å…¥ç¼“å­˜ã€‚
        """
        if sample.extra_info is None:
            sample.extra_info = {}

        cache_file = os.path.join(self.cache_dir, f"{sample.qid}.json")
        
        # --- 1. å°è¯•ä»ç¼“å­˜åŠ è½½ ---
        if os.path.exists(cache_file):
            print(f"Loading cached result for Sample {sample.qid}...")
            try:
                with open(cache_file, 'r', encoding='utf-8') as f:
                    cached_data = json.load(f)
                
                # æ¢å¤æ ¸å¿ƒå­—æ®µ
                sample.extra_info['final_answer'] = cached_data.get('final_answer', "")
                sample.extra_info['messages'] = cached_data.get('messages', [])
                
                # æ¢å¤ retrieved_elements å¯¹è±¡åˆ—è¡¨
                elements_dicts = cached_data.get('retrieved_elements', [])
                restored_elements = []
                for el_dict in elements_dicts:
                    # è¿‡æ»¤æ‰é PageElement å­—æ®µä»¥é˜²æ­¢æŠ¥é”™
                    valid_keys = PageElement.__annotations__.keys()
                    filtered_dict = {k: v for k, v in el_dict.items() if k in valid_keys}
                    restored_elements.append(PageElement(**filtered_dict))
                
                sample.extra_info['retrieved_elements'] = restored_elements
                return sample
            except Exception as e:
                print(f"Error loading cache for {sample.qid}, rerunning inference. Error: {e}")

        # --- 2. æ‰§è¡Œæ¨ç† ---
        print(f"Processing Sample {sample.qid} with Agentic Logic (Inference)...")
        final_answer, history_messages = self.run_agent_loop(sample)

        # è®°å½•ç»“æœåˆ° extra_info
        sample.extra_info['messages'] = history_messages
        sample.extra_info['final_answer'] = final_answer
        
        # --- 3. å†™å…¥ç¼“å­˜ ---
        try:
            # å‡†å¤‡åºåˆ—åŒ– retrieved_elements
            elements_to_save = []
            if 'retrieved_elements' in sample.extra_info:
                for el in sample.extra_info['retrieved_elements']:
                    if hasattr(el, 'to_dict'):
                        elements_to_save.append(el.to_dict())
                    elif isinstance(el, dict):
                         elements_to_save.append(el)

            cache_data = {
                "qid": sample.qid,
                "query": sample.query,
                "final_answer": final_answer,
                "messages": history_messages, # åŒ…å«å¤šæ¨¡æ€ä¿¡æ¯ï¼ŒJSONåºåˆ—åŒ–æ—¶æ³¨æ„ image_url å­—æ®µæ¯”è¾ƒå¤§
                "retrieved_elements": elements_to_save
            }
            
            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump(cache_data, f, ensure_ascii=False, indent=2)
            print(f"Saved result for Sample {sample.qid} to cache.")
            
        except Exception as e:
            print(f"Error saving cache for {sample.qid}: {e}")
        
        return sample

    def save_results(self, excel_path: str = "agent_results_summary.xlsx", json_path: str = "agent_results_summary.json"):
        """
        æ±‡æ€»æ‰€æœ‰æ ·æœ¬çš„å¤„ç†ç»“æœï¼Œå¹¶åˆ†åˆ«ä¿å­˜ä¸º Excel å’Œ JSON æ–‡ä»¶ã€‚
        """
        if not self.loader.samples:
            print("No samples to save.")
            return

        data_rows = []
        for sample in self.loader.samples:
            final_ans = sample.extra_info.get('final_answer', "") if sample.extra_info else ""
            elements_to_save = []
            if 'retrieved_elements' in sample.extra_info:
                for el in sample.extra_info['retrieved_elements']:
                    if hasattr(el, 'to_dict'):
                        elements_to_save.append(el.to_dict())
                    elif isinstance(el, dict):
                         elements_to_save.append(el)
                         
            row = {
                "QID": sample.qid,
                "Query": sample.query,
                "Gold Answer": sample.gold_answer,
                "Model Answer": final_ans,
                "Retrieved Elements": json.dumps(elements_to_save),
                "Data Source": sample.data_source
            }
            data_rows.append(row)

        # --- ä¿å­˜ä¸º Excel ---
        if excel_path:
            try:
                df = pd.DataFrame(data_rows)
                df.to_excel(excel_path, index=False)
                print(f"\nâœ… Excel summary saved to: {excel_path}")
            except ImportError:
                print("Error: pandas or openpyxl not installed. Cannot save to Excel.")
            except Exception as e:
                print(f"Error saving Excel: {e}")

        # --- ä¿å­˜ä¸º JSON ---
        if json_path:
            try:
                with open(json_path, 'w', encoding='utf-8') as f:
                    json.dump(data_rows, f, ensure_ascii=False, indent=2)
                print(f"âœ… JSON summary saved to: {json_path}")
            except Exception as e:
                print(f"Error saving JSON: {e}")

# --- æµ‹è¯•ä»£ç  ---
if __name__ == "__main__":
    from src.loaders.MMLongLoader import MMLongLoader
    from src.agents.ElementExtractor import ElementExtractor
    from src.agents.utils import ImageZoomOCRTool
    
    # 1. æ¨¡æ‹Ÿç¯å¢ƒé…ç½®
    # è¯·ç¡®ä¿ä¿®æ”¹ä¸ºä½ æœ¬åœ°çš„æ­£ç¡®è·¯å¾„
    root_dir = "/mnt/shared-storage-user/mineru3-share/wangzhengren/PageElement/MMLongBench-Doc"
    tool_work_dir = "./workspace"
    
    # å®šä¹‰ç¼“å­˜å’Œç»“æœè¾“å‡ºè·¯å¾„
    cache_dir = "./workspace/cache_mmlong"
    output_excel = None
    output_json = "./workspace/mmlong_results.json"

    # 2. åˆå§‹åŒ–åº•å±‚æå–å™¨
    tool = ImageZoomOCRTool(work_dir=tool_work_dir)
    extractor = ElementExtractor(
        base_url="http://localhost:8001/v1",
        api_key="sk-123456",
        model_name="MinerU-Agent-CK300",
        tool=tool
    )
    
    # 3. åˆå§‹åŒ– Loader
    # ä»…åŠ è½½éƒ¨åˆ†æ•°æ®ç”¨äºæµ‹è¯•
    if os.path.exists(root_dir):
        loader = MMLongLoader(data_root=root_dir, extractor=extractor)
        loader.load_data()
        
        # æˆªå–å‰ 5 ä¸ªæ ·æœ¬è¿›è¡Œæµ‹è¯•
        loader.samples = loader.samples[:5] 

        # 4. åˆå§‹åŒ– Agentic RAG Agent (å¸¦ç¼“å­˜)
        agent = AgenticRAGAgent(
            loader=loader,
            base_url="http://localhost:3888/v1", 
            model_name="qwen3-max",
            api_key="sk-6TGzZJkJ5HfZKwnrS1A1pMb1lH5D7EDfSVC6USq24aN2JaaR",
            max_rounds=5,
            cache_dir=cache_dir
        )
        
        # 5. æ‰¹é‡å¤„ç†
        print(f"\nğŸš€ Starting Batch Processing on {len(loader.samples)} samples...")
        for sample in loader.samples:
            agent.process_sample(sample)
        
        # 6. æ±‡æ€»ç»“æœ
        agent.save_results(excel_path=output_excel, json_path=output_json)
        
    else:
        print(f"Data root {root_dir} does not exist. Skipping test.")