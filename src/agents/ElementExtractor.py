import re
import json
import base64
from io import BytesIO
from PIL import Image
from typing import List, Dict, Optional
from .utils import * # Assuming local_image_to_data_url and TOOL_CALL_RE are here

from openai import OpenAI
import ast
SYSTEM_PROMPT = """You are an advanced Visual Document Analysis Agent capable of precise evidence extraction from document images. Your goal is to answer user queries by locating, reading, and extracting specific information from a page.

### Your Capabilities & Tools

You have access to a powerful tool named **`image_zoom_and_ocr_tool`**.

* **Functionality**:
  Crop a specific region of the image, optionally rotate it, and perform OCR or layout analysis depending on the element type.

* **When to use**:

  * Always use this tool when the user asks for **specific text, numbers, names, dates, tables, equations, images, or factual details** from the page.
  * If the target text or table is rotated, estimate and set the `angle` parameter before cropping.
  * Use `type` to indicate the content type for the region:

    * `"region"` — perform layout detection + OCR on a potentially complex area, returning detailed structured information.
    * `"text"` — perform OCR for a single text element.
    * `"table"` — perform OCR and parsing for a table element.
    * `"image"` — crop the image region only, without OCR.
    * `"equation"` — perform OCR for mathematical or scientific equations.

* **Parameters**:

  * `label`: A short description of what you are looking for.
  * `bbox`: `[xmin, ymin, xmax, ymax]` in **0–1000 normalized coordinates**, relative to the original page.
  * `angle`: Rotation angle (counter-clockwise) applied after cropping. Always try to adjust it so the content is upright for best recognition.
  * `type`: One of `"region"`, `"text"`, `"table"`, `"image"`, `"equation"`.

### Tool Usage Example

Use the tool strictly in the following format:
<think>
...
</think>
<tool_call> 
{"name": "image_zoom_and_ocr_tool", "arguments": {"label": "<A short description of what you are looking for>", "bbox": [xmin, ymin, xmax, ymax], "angle":<0/90/180/270>, "type": "<region/text/table/image/equation>"}}
</tool_call>

### Your Input and Task

The input includes:

1. One page image of a visual document.
2. The user's query intent.

Please execute the following steps:

1. **Semantic Matching**: Carefully observe the image to determine if the page content contains evidence information relevant to the user's query. If it is irrelevant, return an empty list.
2. **Precise Localization**: If relevant, extract the complete chain of visual evidence that helps to answer the query (text blocks, tables, charts, images, or equations).
3. **Special Notes**: The page image may contain several evidence pieces. Pay attention to tables, charts, images, and equations, as they could also contain evidence.

### Output Format

After gathering information, output the list of relevant evidence in the following JSON format.
<think>
...
</think>
```json
[
  {
    "evidence": "<self-contained content, understandable without page context>",
    "bbox": [xmin, ymin, xmax, ymax] # 0-1000 normalized coordinates 
  }
  ...
]
```

If the page image is not relevant, return an empty list.
<think>
...
</think>
```json
[]
```
"""

class ElementExtractor:
    def __init__(self, base_url: str, api_key: str, model_name: str, tool, max_image_size: int = 2048):
        self.client = OpenAI(
            base_url=base_url,
            api_key=api_key,
        )
        self.model_name = model_name
        self.tool = tool
        self.max_image_size = max_image_size

    def compress_image(
        self,
        image_path: str,
        mode: str = "keep_ratio",
        target_size: int = 1000,
    ) -> str:
        """
        Compresses / Resizes an image and returns a base64 data URL.

        Modes:
        - keep_ratio: keep aspect ratio, longest side <= self.max_image_size
        - fixed: resize to (target_size, target_size)
        """
        try:
            with Image.open(image_path) as img:
                width, height = img.size

                if mode == "keep_ratio":
                    if max(width, height) > self.max_image_size:
                        scale = self.max_image_size / max(width, height)
                        new_width = int(width * scale)
                        new_height = int(height * scale)
                        img = img.resize(
                            (new_width, new_height),
                            Image.Resampling.LANCZOS
                        )

                elif mode == "fixed":
                    img = img.resize(
                        (target_size, target_size),
                        Image.Resampling.LANCZOS
                    )

                else:
                    raise ValueError(f"Unknown compress mode: {mode}")

                # Encode to base64
                buffered = BytesIO()
                fmt = "PNG" if img.mode == "RGBA" else "JPEG"
                img.save(buffered, format=fmt, quality=85, optimize=True)

                img_str = base64.b64encode(buffered.getvalue()).decode("utf-8")
                mime_type = f"image/{fmt.lower()}"

                return f"data:{mime_type};base64,{img_str}"

        except Exception as e:
            print(f"[ERROR] Failed to compress image {image_path}: {e}")
            return local_image_to_data_url(image_path)


    def build_multimodal_user_message(
        self,
        text: str,
        image_paths: Optional[List[str]] = None
    ) -> Dict:
        content = []

        if image_paths:
            for path in image_paths:
                # Use the compression method here
                content.append({
                    "type": "image_url",
                    "image_url": {
                        "url": self.compress_image(path,mode="fixed")
                    }
                })

        if text:
            content.append({
                "type": "text",
                "text": text
            })

        return {
            "role": "user",
            "content": content
        }

    def build_multimodal_tool_message(
        self,
        text: Optional[str] = None,
        image_path: Optional[str] = None
    ) -> Dict:
        content = []
        content.append({
                "type": "text",
                "text": "<tool_response>\n"
            })

        if image_path:
            # Use the compression method here
            content.append({
                    "type": "image_url",
                    "image_url": {
                        "url": self.compress_image(image_path,mode="keep_ratio")
                    }
                })

        if text:
            content.append({
                "type": "text",
                "text": f"\n{text}"
            })

        content.append({
                "type": "text",
                "text": "\n</tool_response>"
            })

        return {
            "role": "user",
            "content": content
        }
    def extract_tool_call(self,text: str):
        m = TOOL_CALL_RE.search(text)
        if not m:
            return None

        raw = m.group(1).strip()

        # 找第一个 dict（从第一个 { 到最后一个 }）
        start = raw.find("{")
        end = raw.rfind("}")
        if start == -1 or end == -1 or end <= start:
            return None

        payload = raw[start:end + 1]

        try:
            return json.loads(payload)
        except Exception:
            try:
                return ast.literal_eval(payload)
            except Exception:
                return None

        return None

    # ---------- tool mock ----------

    def _handle_tool_call(self, tool_call, image_path, step, max_rounds) -> str:
        if step >= max_rounds - 2:
            return [False, "You have used up all the available uses of `image_zoom_and_ocr_tool`, please return you final response without use tool."]
        
        result_list = self.tool.call(tool_call, image_path)
        
        if result_list[0] == False:
            result_list = self.tool.call(tool_call, image_path)
            if result_list[0] == False: # Retry once
                return [False, "`image_zoom_and_ocr_tool` is wrong, you can try it again."]
        
        if len(result_list) == 2:
            return [True, result_list[1]]
        
        if len(result_list) == 3:
            return [True, result_list[1], f"{result_list[2]}"]

    # ---------- agent loop ----------

    def run_agent(
        self,
        user_text: str,
        image_paths: Optional[List[str]] = None,
        max_rounds: int = 10,
        uid: int = 1
    ):  
        messages = [
            {"role": "system", "content": SYSTEM_PROMPT},
            self.build_multimodal_user_message(user_text, image_paths),
        ]
        
        # Note: We store original paths in output log, but send compressed to model
        output = {
            "id": uid,
            "predictions": [
                {"role": "system", "content": SYSTEM_PROMPT},
                {"role": "user", "content": "<image>\n" + user_text}
            ],
            "images": [image_paths[0]] if image_paths else []
        }

        for step in range(max_rounds):
            try:
                resp = self.client.chat.completions.create(
                    model=self.model_name,
                    messages=messages,
                    temperature=1.0
                )
            except Exception as e:
                print(f"[ERROR] API Call failed at step {step}: {e}")
                # You might want to break or continue depending on failure strategy
                break

            content = resp.choices[0].message.content

            messages.append({"role": "assistant", "content": content})
            output["predictions"].append({"role": "assistant", "content": content})

            tool_call = self.extract_tool_call(content)
            
            # No tool call -> agent finished
            if tool_call is None:
                return output
            
            # Use original image path for tool execution (to preserve precision),
            # but the visual feedback to the model will be compressed inside build_multimodal_tool_message
            tool_response_list = self._handle_tool_call(tool_call, image_paths[0], step, max_rounds)
            
            if tool_response_list[0] == False:
                messages.append(
                    self.build_multimodal_tool_message(text=tool_response_list[1])
                )
                output["predictions"].append({"role": "user", "content": "<tool_response>\n" + tool_response_list[1] + "\n</tool_response>"})

            if tool_response_list[0] == True and len(tool_response_list) == 2:
                messages.append(
                    self.build_multimodal_tool_message(image_path=tool_response_list[1])
                )
                output["predictions"].append({"role": "user", "content": "<tool_response>\n<image>\n</tool_response>"})
                output["images"].append(tool_response_list[1])

            if tool_response_list[0] == True and len(tool_response_list) == 3:
                messages.append(
                    self.build_multimodal_tool_message(image_path=tool_response_list[1], text=tool_response_list[2])
                )
                output["predictions"].append({"role": "user", "content": "<tool_response>\n<image>\n" + tool_response_list[2] + "\n</tool_response>"})
                output["images"].append(tool_response_list[1])

        print(f"[WARN] uid={uid} exceeded max_rounds, drop this sample")
        return None

if __name__ == "__main__":
    # Ensure ImageZoomOCRTool is imported or defined
    # from .tools import ImageZoomOCRTool 

    agent = ElementExtractor(
        base_url="http://localhost:8000/v1",
        api_key="no-key-required",
        model_name="/mnt/shared-storage-user/mineru2-shared/madongsheng/saves/Qwen3-VL-8B-Instruct/full/demond_0203_f_base/checkpoint-400",
        tool=ImageZoomOCRTool(work_dir="/mnt/shared-storage-user/mineru2-shared/madongsheng/zoom"),
        max_image_size=2048 # Limit longest edge to 2048px
    )
    
    file_path = "/mnt/shared-storage-user/mineru2-shared/jiayu/data/FinRAGBench-V/data/citation_labels/citation_labels_new/finragbench_bbox_test.json"

    # Read JSON file
    with open(file_path, "r", encoding="utf-8") as f:
        inputs = json.load(f)
    
    item = inputs[0]
    # Defensive check for content structure
    if "conversations" in item and len(item["conversations"]) > 0:
        content_text = item["conversations"][0].get("content", "")
        # Assuming format like "<image>\nQuery..."
        user_text = content_text[8:] if content_text.startswith("<image>") else content_text
    else:
        user_text = "Analyze this page."
        
    image_paths = [item["images"][0]]
    
    results = agent.run_agent(user_text, image_paths)
    
    print(results)